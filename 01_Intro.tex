\section{Introduction}
\vspace{-5pt}
% no \IEEEPARstart
The amount of data produced by affordable edge devices and sensors has been increasing exponentially over the recent past, thanks to the increasing pervasiveness of the Internet of Things (IoT) paradigm, the growing interest in Smart City applications, and the gradual deployment of 5G networks \cite{sta2017quality,cheng2015building,8493126}. 
%Cisco predicts that almost two thirds of the global population will have Internet access by 2023 with the total number of Internet users growing from 3.9 billion in 2018 to 5.3 billion by 2023. 
The amount of data traffic is poised to grow faster than the number of connections because of the increased use of data hungry applications, such as telemedicine and smart driving systems \cite{cisco2020cisco}. Much of this data is key for enabling decision making so as to improve the behavior of complex systems. For example, vehicle trajectory prediction creates new opportunities for improving transport services and reducing traffic congestion \cite{8336896}. An increasing number of services and applications in the vehicular domain rely on trajectory data and AI/ML (artificial intelligence, machine learning) techniques to improve their effectiveness and efficiency.\\
Standard ML algorithms rely on training over datasets built  from a large number of sources, and typically stored centrally, on one machine or in a data center. Storing such data in a central place has become more and more problematic because of data protection rules, and customer privacy concerns. In addition, collecting data from different devices can be inefficient and their transfer can quickly clog the available bandwidth. 
%Hence, conventional centralized ML approaches are becoming less and less attractive.\\
Several distributed ML approaches have been proposed to overcome these limitations \cite{smith2017federated,kraska2013mlbase}. Among these, Federated Learning (FL), first introduced by Google \cite{mcmahan2017communication}, %, is particularly interesting for vehicle trajectory prediction. FL 
is based on a synchronous coordinator-client (server-client) architecture. FL collaboratively learns a single consensus model for all clients from decentralized data without the need to store data centrally. Data remains where it was generated, which guarantees privacy and reduces communication cost. 
%Centralized FL algorithms operate based on a central coordinator/server that organises the different steps of the algorithm and performs as a reference clock. In centralized FL approaches, the server initially distributes to clients a model to be trained. Then, at each iteration, the server randomly selects only a few nodes to collect their model instances trained on local data. 
%They have non-iid data that also varies in quantity.
%After some training rounds, the central server generates a global model that is distributed to all clients. 
However, learning heavily depends on the coordinating server, which causes scalability issues with large numbers of nodes. %Moreover, such architecture implies a single point of failure, which is not suitable for applications in which availability is key.
\\Decentalized ML algorithms have been proposed to tackle scalability issues. In these algorithms, learning is implemented collaboratively amongst all nodes with no central server, aggregator or coordinator. One of the state-of-the-art approaches in this field is Gossip Learning (GL) \cite{ormandi2013gossip}, which implements a decentralized version of FL. Each node in this distributed algorithm acts as a client for other nodes. At the same time, it can act as a coordinating server that merges received models. GL has been applied to different ML problems. In \cite{blot2016gossip}, a gossip protocol is used in which local models are distributed over a logically fully connected peer-to-peer network serving an application of distributed learning for medical data centres. However, the solution has scalability and connectivity issues of its own. In \cite{hu2019decentralized}, a segmented gossip aggregation is introduced. The global model is divided into non-overlapping subsets. Local learners aggregate the segmentation sets from other learners.  The proposed approach is application-dependent and not suitable for more general machine learning contexts. Savazzi et al.\cite{savazzi2020federated} proposed a fully serverless FL approach, in which nodes receive a combined model from their neighbours, and each one independently performs training steps on its local dataset. Then, similarly to \cite{blot2016gossip}, nodes forward updated models to their one-hop neighbourhood for a new consensus step. Their goal is exploiting a serverless consensus paradigm for FL and enhancing the speed of convergence. Individual model instances are not personalized to increase local performance. Moreover, the majority of existing GL approaches consider  scenarios  in  which  either each  node communicates with all other nodes, or the connectivity graph is  static. By not accounting for churn and mobility, these approaches are not applicable in dynamic setups such as vehicular ad hoc networks (VANETs).\\
In this work we consider a scenario in which the learning agents are vehicles moving in an urban setting, and exchanging information directly in an ad-hoc mode, e.g. via cellular D2D, WiFi direct, or DSRC \cite{asadi2014modeling,abboud2016interworking}. We consider the case in which each vehicle has to train an LSTM model in order to predict in an online manner its own trajectory, for purposes of vehicular traffic management, or for the implementation of coordinated driving, or for enabling proactive resource allocation algorithms, e.g. in Mobile Edge Computing (MEC) schemes \cite{9149032}.\\ 
We propose a decentralized GL scheme which is online, peer-to-peer and based on asynchronous communications. Each node in this network uses its local dataset to improve the model instances of nodes it meets opportunistically. At the same time, each node acts as a coordinating server that merges received models in order to improve the performance of its own personalized model instance.\\ 
%There can be as many models as the number of clients.
% Our intended application is a scenario in which a Mobile Network Operator (MNO) receives regularly predictions of car trajectory in order to implement proactive strategies for resource allocation, e.g. for Mobile Edge Computing (MEC) services.\\ 
We present three practical algorithms, called DFed Avg, DFed Pow and DFed MinLoss, to personalize the model instance of each node, based on iterative model averaging. 
%In dataset in our hand nodesâ€™ datasets are varied in size and pattern. 
We evaluate our approach considering a dynamic time series data set.
%(new data samples are %added to the network during learning steps, and nodes 
%collected by nodes as time progresses) and an LSTM model.\\
We perform our numerical experiments over measurement-based mobility traces in an urban setting. Result over a clean-slate scenario suggest that these approaches already perform well when vehicles spend at least about 20 minutes in the considered area, even with dynamic, unbalanced and non-IID data distributions.\\
% Our numerical experiments over measurement-based mobility traces in an urban setting suggest that these approaches perform well on vehicles which spend a sufficiently long amount of time (about 20 minutes at least) in the considered area even with dynamic, unbalanced and non-IID data distributions.\\
The rest of this paper is organized as follows: Section~\ref{sec:System model} describes the system model. In Section~\ref{sec:A distributed architecture for Gossip Learning}, GL algorithms are described in details. Section~\ref{sec:Numerical Evaluation} is dedicated to numerical evaluation and results, and finally future work is discussed in Section~\ref{sec:Conclusion}.



% are learning when we have numerous clients with the . 
%\com{put here a short state of the art}
%\com{include a short statement of what are our contributions in this work?}
%\vspace{-6.5pt}
